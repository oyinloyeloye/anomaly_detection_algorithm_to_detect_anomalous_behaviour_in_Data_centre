# -*- coding: utf-8 -*-
"""anomaly_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oUr5V3evRyKVv8gJrmc3BLdcEKgEuJAc

Topic: Anomaly Detection Algorithm To Detect Anomalous Behavior In Data centers
"""

import pandas as pd
import numpy as np

# load the dataset
df = pd.read_csv('assignment.csv')

print(df)

print(df.info())

print(df.describe())

"""handling missing data in the features

from the data information we can clearly see that we have missing some missing values majorly in the network_type column
"""

df['network_type'].value_counts()

"""Why is it only the network was the one we checked for value_counts cos its the only one with missing values right??
why did we focus on network type, why not another feature?

"""

#check the percentage of each features that can exist

percentage_non_null = df.notnull().mean() * 100
print(percentage_non_null)

"""from the analysis, we can see that 22% of the network_type feature is available, meaning we have 78% of missing data from that feature which is a significant figure.
To re-iterate the network_type feature represents the type of network from which a login attempt was made. Examples include 'hosted', 'mobile' , 'cable'. etc. It is a categorical feature that provide context about the login attempt's origin, which is valuable for detecting patterns and anormalies.

A high volume of missing data is a key feature that can skew analysis and model performance if not handles properly.

To solve this issue, the most frequesnt category, hosted represtents a large portion of the existing data (61,704 occurencies). This suugests that 'hosted' is a common network type for login attempots.

Inputting missing values with 'hosted' assumes that missing entries might follow the same distributiin as observed data, which is often reasonable assumption in large datasets.

**Maintaining Data Integrity**



*   By filling missing values with 'hosted', we ensure that the dataset remains consistent anmd complete, which is crucial for training accurate and reliable models.

*   This approach avoids introducing biases that might occur if anin appropiate value were used.


*   Consistence in data helps in anomaly detection. Having a large number of missing values can complicate the detection process.






"""

# make a copy of the dataset

''' It's a good practice to make a copy of dataset while making external or additional changes
    To preserve the entities and relatioship in the original dataset.

'''
df_new = df.copy()
df_new.info()

# handling missing values  - fill missing values in the network  type with 'hosted'
# Hosted consistutes 51% of the 'network_type' values

df_new['network_type']  = df_new['network_type'].fillna('hosted')
df_new.info()

"""We already have 99% of the country data available , precisely 19 missing value in the data. Since this is not much of a number, we can take away entities that have missing country features"""

# remove rows with missing 'country' columns

df_new = df_new.dropna(subset=['country'])
df_new.info()

df_new

"""from the data dictionary we don't need the unnamed column so we can drop that

"""

# from the unnamed column
df_new = df_new.drop('Unnamed: 0', axis =1)
df_new

"""EXPLORATION DATA ANALYSIS

Encoding and creating some more new features from the provided data
"""

df_new['network_type'] = df_new['network_type'].astype('category').cat.codes
df_new

df_new['network_type'].value_counts()

"""Here, we have nmerical encodings to our dataset using lable encoder. This isessential for the Machine learning models to work

Feature Engineering

Feature Engineering is a crucial step in the data preparation process, especially for anomaly detection and identifying patterns in login attempts.
"""

df_new['hashed_ip']

df_new['login_attempts_per_ip'] = df_new.groupby('hashed_ip')['hashed_ip'].transform('count')
df_new['login_attempts_per_ip']

"""I wannt to calculate the number of unique email addresses that hsve been used from a single IP address attempting to log in wth many different email addresses is suspicious and indicative of an attack trying various credentials.

Behavioural insights : It helps in distinguishing between normal user behaviour(typically logging in with a few emails) and potential attack behaviour (logging in with different emails)
"""

df_new['unique_emails_per_ip'] = df_new.groupby('hashed_ip')['email_hash'].transform('nunique')
df_new['unique_emails_per_ip']

"""explanation of the output: The hashed_ip at row 1 tried 847 different email addresses

I want to calculate the success rate of login attempts for each email address. A low success rate for an email could indicate that many failed attempts are being made, suggesting the account might be a target of credential stuffing.
It helps in identifying emails with abnormal login attempts, such as failed attempts followed by a success, which could indicate that an attacker has eventually found the correct crediatials.
"""

df_new['success_rate_per_email'] = df_new.groupby('email_hash')['status_code'].transform(lambda x: (x == 200).mean())
df_new['success_rate_per_email']

"""example explanation : hashed_email at index 0 has a successs rate of 11.11%"""

# taking a look at the data again

df_new

df_new.info()

"""Now the exploratory part!"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribution analysis

plt.figure(figsize=(12,6))
sns.histplot(df_new['login_attempts_per_ip'], bins = 50)
plt.title('Distribution of Login Attempts per IP')
plt.show()

# Geographical analysis
plt.figure(figsize = (12,6))
country_counts = df_new['country'].value_counts().head(20)
sns.barplot(x=country_counts.index, y = country_counts.values)
plt.title('Top 20 countries by login attempts')
plt.xticks(rotation=45)
plt.show()

# tool and network analysis
plt.figure(figsize=(12,6))
tool_counts = df_new['tool_id'].value_counts().head(20)
sns.barplot(x=tool_counts.index, y=tool_counts.values)
plt.title('Top 20 Tools by Login Attempts')
plt.xticks(rotation=45)
plt.show()

"""Since the tool_id are hashed, I might not have detailes conclusion from the plot above, but recall from the data dictionary:

Tool_id description: Unique identifier of the tool used o submi the login.
Significabce : Different toold might be used for lrgitimate logins versus attck(e,g broeser vs automated scripts)

3. **Anomaly Detection**

Anomaly detection is the process of identifying rare or unusal patters in data that do not conform with the expected behaviour. In the context of detecting malicious login attempts, anomaly detection helps to identify suspicious activities that differ significantly from typical login patterns, such as an unusual high number from a single IP address or numerous failed login attempts for a particulare email.

**Using Unsupervised learning**: Unsupervised learning in a type of ML where the algorithm is trained on unlabelled  data. The goal is to identify patterns and structure in the  data without prior knowlegde of the output labels. In a supervised learning, the model tried to learn the underllying distribution and structure of he data to find the hidden patterns or groupings.

**Relationship to Anomaly Detection**: I can apply unsupervised learning in Anomaly detection because:


*   The dataset typically lacks labels indicating which data points are normal and which are anomalies.
*   The anomalies are rare and diverse, making it challenging to use supervised learning methods  that require labeled examples for each class.

3.5 **Algorithm to be used**

**Isolation Forest**

Isolation Forest(iForest) is an algorithm specifically designed for anomaly detection using Binary Trees. The algorithm has a linear time complexity and a low memory requitement, which works wells with high-volume data. In essesnse, the algorithm relies upon the characteristics of anomalies i.e., being few and different , in order to detect anomalies.



*   Isolation: Anomalies are isolated easier than normal points due to their distinct and sparse characteristics.

*   Forest : It builds multiple decision trees to isolate anomalies. Each tree is constructed by randomly selecting feature and then randomly selecting a split value between the maximum and minimun values of the selected feature.

The process isolates ano,malies faster because anomalies are typically found in fewer in split than normal points. The algorithm's effectiveness comes from the ability to quickly isolate anomalies , making it compututationally effecient and effective for large datasets
"""

from sklearn.ensemble import IsolationForest
# selecting featiress for anomaly detection

features = ['login_attempts_per_ip', 'unique_emails_per_ip',  'success_rate_per_email' , 'network_type']

# Initialise the model

model = IsolationForest(contamination=0.01)

# fit the model

df_new['anomaly'] = model.fit_predict(df_new[features])

# mark anomalies

df_new['anomaly'] = df_new['anomaly'].apply(lambda x: 'malicious' if x == -1 else 'legitimate')

df_new['anomaly']

df_new['anomaly'].value_counts()

"""**Explanation**

1. I selected the features that I believe are the most important for the anomaly detection process.


*   login_attempts_per_ip : recall a high login attempt from the resultant calculation that indicated suspicious attempts.

*   unique_emails_per_ip : This is important because if a single IP has plenty unique emails, it is suspicious.

*   success_rate_per_email : This is omportant because we know the emial that don't have good percentage success rate based on their staus code.


*   network_type : This is important because we have the type of network that login atatempts were made from  and we had already numerically encoded this.

2. I initialised the model and added contamination parameter: This parameter indicates teh proportion of the dataset that is expected to be anomalies. In thios case , 1% of the data is assumed  to be anomalous because of the distribution if the login attempts per IP, jsut few have suspicous counts of high login attempt per IP, we can also further iterate on these parameters.

3. I fit the model initialised to the respective features defined to predict if each entities are anomalies or not, then created a new column df_new['anomaly'] to store the resilt, this stores 1 for normal points and -1 becomes malicious

4. Detect compromised credentials (emails with successful malicious logins)

Next we waant to detect, based on out model result if we have   any compromised credentials, compromised credentials are entiities taht have status_code = 200(meaning they were successful login attempts) and the model detected them as maliicious despite the successful login, which indicates something fishy in that entity.
"""

compromised_credentials = df_new[(df_new['anomaly'] == 'malicious') & (df_new['status_code'] == 200)]['email_hash']

# add a column to amrk compromised credentials
df_new['compromised_email'] = df_new['email_hash'].apply(lambda x: 'compromised' if x in compromised_credentials else 'safe')

df_new['compromised_email'].value_counts()

"""from this, we can see that we have 10 compromised email

5. outputting the refined CSV
"""

# save the labelled dataset to a CSV file

df_new.to_csv('finalised_assessment_oyinloye.csv', index=False)

# preview the new csv

finalised_df = pd.read_csv("finalised_assessment_oyinloye.csv")
finalised_df

"""In this project, I have successfully implemented an anomaly detection system to identify and flag malicious login atten=,pts usind IsolationForest. By carefully engineering features such as the number of login attempts per IP, the numbber of different emails  tried by the same IP, the success rrate of login for each email and incorporating an external risk indication, we improved the model's ability to detect anomalies.

Our model identifies 2908 amlicious logins out of a total of 530,797, indicating a small but significant presence of potentially harmful activities.

Furthermore, we identified 10 compromisd emails accounts, highlighting the effectiveness of our approach in pinpointiong targeted credential stuffing attacks. These results demonstate the utility of unsupervised learning techniques in enhancing cybersecurity measures and protecting user accounts from auauthorised access.


"""